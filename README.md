
# Breast Tumour Analysis with Explainable AI (XAI)

This project performs **breast tumour classification** using histopathological images and integrates **Explainable AI (Grad-CAM, LIME)** to visualize and interpret model decisions.  It leverages the **BreakHis dataset** and deep learning architectures (CNNs / Vision Transformers) to build interpretable diagnostic systems.

---

## Setup

### Create a Python Virtual Environment
```bash
python -m venv venv
source venv/bin/activate      # macOS/Linux
# On Windows:
venv\Scripts\activate
````

### Install Requirements

```bash
pip install -r requirements.txt
```

---

## ðŸ§¬ Dataset â€” BreakHis

**Source:** [BreakHis Dataset on Kaggle](https://www.kaggle.com/datasets/tathagatbanerjee/breakhis-breast-cancer-histopathological)
**Images:** 7,909 histopathology images
**Magnifications:** 40Ã—, 100Ã—, 200Ã—, 400Ã—
**Classes:** Benign / Malignant (with subtypes)

| Tumor Type    | Subtypes                                                                                         |
| ------------- | ------------------------------------------------------------------------------------------------ |
| **Benign**    | Adenosis (A), Fibroadenoma (F), Phyllodes Tumor (PT), Tubular Adenoma (TA)                       |
| **Malignant** | Ductal Carcinoma (DC), Lobular Carcinoma (LC), Mucinous Carcinoma (MC), Papillary Carcinoma (PC) |

Each image filename encodes biopsy method, class, subtype, patient ID, and magnification.
Example:
`SOB_B_TA-14-4659-40-001.png` â†’ Benign, Tubular Adenoma, 40Ã— magnification.

---

## âš™ï¸ Environment Requirements

* Python â‰¥ 3.9
* Framework: **PyTorch** (preferred) or TensorFlow
* Key libraries:

  * `torch`, `torchvision`
  * `scikit-learn`, `albumentations`
  * `grad-cam`, `lime`
  * `streamlit` (for deployment/visualization)

---

## Project Workflow

### Step 1: Dataset Splitting â€” `split_by_patient.py`

* Shuffle and split patients into **train/val/test = 70/10/20**.
* Copy images into structured folders under `processed/`.
* Generate `manifest.csv` containing metadata for all images.
* Final counts:

  * **Train:** 5,463 images
  * **Val:** 733 images
  * **Test:** 1,713 images

---

### Step 2: Image Pre-processing â€” `preprocess_images.py`

* Resize every image to **224Ã—224 px**.
* Convert to **RGB (3 channels)**.
* Save into `data/preprocessed/` maintaining the same split hierarchy.

---

### Step 3: Dataset & DataLoader (PyTorch)

**Transforms**

```python
# Training
Resize(224, 224)
RandomHorizontalFlip()
RandomVerticalFlip()
ToTensor()
Normalize(mean=ImageNet_mean, std=ImageNet_std)

# Validation/Test
Resize(224, 224)
ToTensor()
Normalize(mean=ImageNet_mean, std=ImageNet_std)
```

**Dataset Loading**

```python
train_ds = ImageFolder(DATA_ROOT / "train", transform=train_tfms)
val_ds = ImageFolder(DATA_ROOT / "val", transform=eval_tfms)
test_ds = ImageFolder(DATA_ROOT / "test", transform=eval_tfms)
```

**Class Distribution**

| Split | Total | Benign | Malignant |
| ----- | ----- | ------ | --------- |
| Train | 5463  | 1622   | 3841      |
| Val   | 733   | 190    | 543       |
| Test  | 1713  | 668    | 1045      |

Observed class imbalance â†’ handled during training via sampling or weighted loss.

**DataLoaders**

```python
train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)
val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)
test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=4)
```

Verified batch shape `[32, 3, 224, 224]` and correct label mapping.

---

### Model Overview

This project leverages **deep transfer learning** to classify breast tumor histopathological images into **benign** and **malignant** categories.
Three pre-trained CNN architectures were evaluated and compared for performance:

1. **ResNet-50** â€“ served as a strong baseline model for feature extraction and binary classification.
2. **DenseNet-121** â€“ provided deeper gradient flow and improved feature reuse, achieving competitive accuracy and macro-F1 scores.
3. **KimiaNet** â€“ a specialized model pre-trained on large-scale **histopathology datasets**.

   * Unlike generic ImageNet models, KimiaNet is trained to capture **microscopic tissue-level features**, making it highly effective for medical image analysis.
   * In this project, KimiaNet demonstrated superior performance with an accuracy of **86.6%** and a macro-F1 score of **85.1%** on the test set.

---

### Using KimiaNet Weights

To use KimiaNet in this project, you must first download the pre-trained weights file:
**`KimiaNetPyTorchWeights.pth`**

You can download the weights from the following source:
[KimiaNet Weights â€“ Download Link](https://github.com/KimiaLabMayo/KimiaNet/tree/main/KimiaNet_Weights/weights)

Once downloaded, place it in the directory:

```
weights/KimiaNetPyTorchWeights.pth
```

The model loading function will automatically load these weights during initialization:

```python
model = build_kimianet(weights_path="weights/KimiaNetPyTorchWeights.pth", num_classes=2)
```

---

## Folder Structure

```
breast-tumour-analysis-xai/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ preprocessed/
â”‚
â”œâ”€â”€ runs/
â”‚   
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ split_by_patient.py
â”‚   â”œâ”€â”€ preprocess_images.py
â”‚   â”œâ”€â”€ train.py
â”‚
â”œâ”€â”€ weights/
â”‚   â””â”€â”€ model.pt  # (ignored in .gitignore)(kimianet weights)
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## Explainable AI (XAI)

### Techniques Used:

* **Grad-CAM** : Visualizes important regions influencing model predictions.
* **LIME** : Generates local explanations for individual predictions.

Helps in validating model reliability and improving transparency in clinical contexts.

### Understanding Model Decisions

To interpret model predictions, Explainable AI (XAI) techniques such as Grad-CAM and LIME were applied. These visualization methods reveal which regions of histopathology images influenced the modelâ€™s decision, enhancing interpretability and trust in the diagnostic process. First experimented on a single test image to interpret KimiaNetâ€™s decision-making process using Grad-CAM and LIME. 

```python
src\gradcam_kimianet_singletest.py
src\gradcam+lime_kimianet_singletest.py
```

### Step 1: Grad-CAM (Gradient-weighted Class Activation Mapping)

Grad-CAM generates a heatmap over the input image, highlighting the most influential areas contributing to the prediction.

In this analysis, a test image from the malignant class was processed through the trained KimiaNet model.
The Grad-CAM output indicated that:

* The modelâ€™s attention followed tissue patterns rather than background noise.

* High-activation zones aligned with cellular and structural boundaries.

Example Visualization:

![Grad-CAM Visualization](runs/singletest_outputs/Grad-CAM.png)

* The visualization confirms that the model focuses on clinically relevant regions.

* The Grad-CAM heatmap shows smooth, spatially coherent regions of red/yellow overlay. These correspond to:

* High-attention zones where the networkâ€™s feature maps have high activation.

* In this case, those red/orange streaks follow dense epithelial glands with irregular, hyperchromatic nucleiâ€”classic malignant morphology in ductal carcinoma.

* Cooler regions (blue/purple) represent connective stroma or empty lumens, which are less diagnostically relevant.

Conclusion: Grad-CAM is focusing on histologically meaningful tumor structures â€” a strong qualitative validation that the modelâ€™s attention aligns with true malignant regions.
	
### Step 2: LIME (Local Interpretable Model-Agnostic Explanations)

LIME was used to further interpret model behavior on the same image. It perturbs image regions (superpixels) and analyzes their influence on prediction confidence.

LIME divides the image into super-pixels and perturbs them to see which ones most change the prediction.

Example Visualization:

![GradCAM+LIME Visualization](runs/singletest_outputs/GradCAM+LIME.png)

Here:

* Yellow outlines encircle compact glandular clusters with high nuclear density.

* The algorithm identifies multiple discrete â€œpatchesâ€ within the malignant areas.

* LIMEâ€™s selections overlap quite well with the high-intensity Grad-CAM regions, though theyâ€™re more fragmented â€” expected, since LIME operates at the patch level rather than via feature maps.

Conclusion:
LIME and Grad-CAM are consistent â€” both emphasize similar epithelial tumor zones rather than background tissue.
This convergence increases confidence that the model isnâ€™t relying on artifacts (like slide corners or staining intensity) but on morphologic cues actually indicative of malignancy.


### Rationale for Single-Image Analysis

The single-image analysis stage was performed to:

* Validate the Grad-CAM and LIME visualization pipelines before applying them to all test samples.

* Qualitatively assess focus areas and interpretability.

* Ensure consistency in color maps, normalization, and visualization settings.

After verification, the process was scaled to 40 test samples for a comprehensive evaluation of both benign and malignant categories.

---

## Citation

If you use this repository or the BreakHis dataset, please cite:

> Spanhol, F.A., Oliveira, L.S., Petitjean, C., & Heutte, L.
> "A Dataset for Breast Cancer Histopathological Image Classification."
> *IEEE Transactions on Biomedical Engineering*, 2016.

---
